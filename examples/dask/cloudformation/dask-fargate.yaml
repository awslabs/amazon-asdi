Parameters:
  DaskWorkerGitToken:
    Type: String
    Description: GitHub OAuth Token for accessing the Dask worker repository
    MinLength: 40
    AllowedPattern: "[a-zA-Z0-9]+"
    NoEcho: true

  SagemakerCodeRepo:
    Type: String
    Description: Github Repository for loading into the Sagemaker Jupyter environment
    Default: https://github.com/awslabs/amazon-asdi.git

  DaskWorkerGitSourceRepo:
    Type: String
    Description: GitHub Repository for building the container image for the Dask workers
    Default: amazon-asdi

  DaskWorkerGitHubOrg:
    Type: String
    Description: GitHub organization for Dask Worker repository
    Default: awslabs

  DaskWorkerGitSourceBranch:
    Type: String
    Description: GitHub Repository Branch for Dask workers
    Default: dask

  CodeBuildDockerImage:
    Type: String
    Default: aws/codebuild/standard:5.0

  InitialImage:
    Type: String
    Default: daskdev/dask:2.9.2
    Description: Initial image to use for the Dask workers before our custom image is built

Resources:
  CodePipelineArtifactBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete

  CleanBucketOnDelete:
    DependsOn: CleanBucketOnDeleteLambda
    Type: Custom::CleanBucket
    Properties:
      ServiceToken: 
        Fn::GetAtt: 
          - "CleanBucketOnDeleteLambda"
          - "Arn"
      BucketName: !Ref CodePipelineArtifactBucket

  CleanBucketOnDeleteLambda:
    DependsOn: CodePipelineArtifactBucket
    Type: "AWS::Lambda::Function"
    Properties:
      Code: 
        ZipFile: 
          !Sub |
            import json, boto3, logging
            import cfnresponse
            logger = logging.getLogger()
            logger.setLevel(logging.INFO)
    
            def lambda_handler(event, context):
                logger.info("event: {}".format(event))
                try:
                    bucket = event['ResourceProperties']['BucketName']
                    logger.info("bucket: {}, event['RequestType']: {}".format(bucket,event['RequestType']))
                    if event['RequestType'] == 'Delete':
                        s3 = boto3.resource('s3')
                        bucket = s3.Bucket(bucket)
                        for obj in bucket.objects.filter():
                            logger.info("delete obj: {}".format(obj))
                            s3.Object(bucket.name, obj.key).delete()
    
                    sendResponseCfn(event, context, cfnresponse.SUCCESS)
                except Exception as e:
                    logger.info("Exception: {}".format(e))
                    sendResponseCfn(event, context, cfnresponse.FAILED)
    
            def sendResponseCfn(event, context, responseStatus):
                responseData = {}
                responseData['Data'] = {}
                cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")            
    
      Handler: "index.lambda_handler"
      Runtime: python3.7
      MemorySize: 128
      Timeout: 60
      Role : !GetAtt CleanBucketOnDeleteLambdaRole.Arn

  CleanBucketOnDeleteLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: !Join ['-', [!Ref 'AWS::StackName', 'CleanBucketOnDeleteLambdaPolicy'] ]
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:ListBucket
            - s3:DeleteObject
            Resource:
            - !GetAtt CodePipelineArtifactBucket.Arn
            - !Join
              - "/"
              - - !GetAtt CodePipelineArtifactBucket.Arn
                - "*"
          - Effect: Deny
            Action:
            - s3:DeleteBucket
            Resource: '*'
          # - Effect: Allow
          #   Action:
          #   - logs:CreateLogGroup
          #   - logs:CreateLogStream
          #   - logs:PutLogEvents
          #   Resource: '*'

  CodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Resource:
                  Fn::GetAtt:
                    - CodeBuildLogs
                    - Arn
                Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
              - Resource: "*"
                Effect: Allow
                Action:
                  - ec2:CreateNetworkInterface
                  - ec2:DescribeDhcpOptions
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DeleteNetworkInterface
                  - ec2:DescribeSubnets
                  - ec2:DescribeSecurityGroups
                  - ec2:DescribeVpcs
                  - ec2:CreateNetworkInterfacePermission
                  - ecr:GetAuthorizationToken
              - Resource: !Sub arn:aws:s3:::${CodePipelineArtifactBucket}/*
                Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:GetObjectVersion
              - Resource: !GetAtt EcrDockerRepository.Arn
                Effect: Allow
                Action:
                  - ecr:GetDownloadUrlForLayer
                  - ecr:BatchGetImage
                  - ecr:BatchCheckLayerAvailability
                  - ecr:PutImage
                  - ecr:InitiateLayerUpload
                  - ecr:UploadLayerPart
                  - ecr:CompleteLayerUpload

  
  CodeBuildLogs:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/codebuild/${AWS::StackName}
      RetentionInDays: 3
    UpdateReplacePolicy: Delete
    DeletionPolicy: Delete

                  # By default, the build specification is defined in this template, but you can also add buildspec.yml
  # files in your repos to allow for customization.
  # See:
  # https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html
  # https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-source.html
  CodeBuildProject:
    DependsOn: CodeBuildLogs
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: CODEPIPELINE
      Source:
        Type: CODEPIPELINE
        BuildSpec: !Sub
          - |
            ---
            version: 0.2
            phases:
              install:
                commands:
                  - nohup /usr/local/bin/dockerd --host=unix:///var/run/docker.sock --host=tcp://127.0.0.1:2375 --storage-driver=overlay2&
                  - timeout 15 sh -c "until docker info; do echo .; sleep 1; done"
              pre_build:
                  commands:
                  - printenv
                  - TAG="$REPOSITORY_NAME.$REPOSITORY_BRANCH.$(date +%Y-%m-%d.%H.%M.%S).$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | head -c 8)"
                  - echo $TAG
                  - aws ecr get-login-password --region "$REGION" | docker login --username AWS --password-stdin "$ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com"
              build:
                commands:
                  - cd examples/dask-worker && docker build --tag $REPOSITORY_URI:$TAG . && cd ../..
              post_build:
                commands:
                  - docker push $REPOSITORY_URI:$TAG
                  - printf '[{"name":"${ServiceName}","imageUri":"%s"}]' $REPOSITORY_URI:$TAG > build.json
            artifacts:
              files: build.json
          - ServiceName: Worker
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Type: LINUX_CONTAINER
        PrivilegedMode: True
        Image: !Ref CodeBuildDockerImage
        EnvironmentVariables:
          - Name: ACCOUNT_ID
            Value: !Sub ${AWS::AccountId}
          - Name: REGION
            Value: !Sub ${AWS::Region}
          - Name: REPOSITORY_URI
            Value: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${EcrDockerRepository}
          - Name: REPOSITORY_NAME
            Value: !Ref DaskWorkerGitSourceRepo
          - Name: REPOSITORY_BRANCH
            Value: !Ref DaskWorkerGitSourceBranch
      Name: !Ref AWS::StackName
      ServiceRole: !Ref CodeBuildServiceRole

  CodePipelineServiceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: codepipeline.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: codepipeline-access
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Resource: "*"
                Effect: Allow
                Action:
                  - ecs:List*
                  - ecs:Describe*
                  - ecs:RegisterTaskDefinition
                  - ecs:UpdateService
                  - codebuild:StartBuild
                  - codebuild:BatchGetBuilds
                  - codecommit:GetBranch
                  - codecommit:GetCommit
                  - codecommit:UploadArchive
                  - codecommit:GetUploadArchiveStatus
                  - codecommit:CancelUploadArchive
                  - iam:PassRole
              - Resource: !Sub arn:aws:s3:::${CodePipelineArtifactBucket}/*
                Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:GetObjectVersion
                  - s3:GetBucketVersioning
              - Resource: !Sub ${NotifyWaitConditionLambdaFunction.Arn}
                Effect: Allow
                Action:
                  - lambda:InvokeFunction
                  - lambda:InvokeAsync
    DependsOn:
      - CodePipelineArtifactBucket

  # This CodePipeline triggers on a commit to the Git branch passed, builds the Docker image
  # and then deploys the container in the Fargate Cluster.
  CodePipelineGitHub:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      RoleArn: !GetAtt CodePipelineServiceRole.Arn
      ArtifactStore:
        Type: S3
        Location: !Ref CodePipelineArtifactBucket
      Stages:
        - Name: Source
          Actions:
            - Name: App
              ActionTypeId:
                Category: Source
                Owner: ThirdParty
                Version: 1
                Provider: GitHub
              Configuration:
                Owner: !Ref DaskWorkerGitHubOrg
                Repo: !Ref DaskWorkerGitSourceRepo
                Branch: !Ref DaskWorkerGitSourceBranch
                OAuthToken: !Ref DaskWorkerGitToken
              OutputArtifacts:
                - Name: App
              RunOrder: 1
        - Name: Build
          Actions:
            - Name: Build
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref CodeBuildProject
              InputArtifacts:
                - Name: App
              OutputArtifacts:
                - Name: BuildOutput
              RunOrder: 1
        - Name: Deploy
          Actions:
            - Name: DeployScheduler
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Version: 1
                Provider: ECS
              Configuration:
                ClusterName: !Ref DaskCluster
                ServiceName: !GetAtt DaskSchedulerService.Name
                FileName: build.json
              InputArtifacts:
                - Name: BuildOutput
              RunOrder: 1
            - Name: DeployWorker
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Version: 1
                Provider: ECS
              Configuration:
                ClusterName: !Ref DaskCluster
                ServiceName: !GetAtt DaskWorkerService.Name
                FileName: build.json
              InputArtifacts:
                - Name: BuildOutput
              RunOrder: 2
            - Name: NotifyCloudformation
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Version: "1"
                Provider: Lambda
              Configuration:
                FunctionName: !Ref NotifyWaitConditionLambdaFunction
              RunOrder: 3
          
    DependsOn:
      - CodePipelineArtifactBucket
      - CodeBuildProject
      - CodePipelineServiceRole

  DeployWaitHandle:
    Type: AWS::CloudFormation::WaitConditionHandle

  DeployWaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    Properties:
      Count: 1
      Handle: !Ref DeployWaitHandle
      Timeout: "900"
  
  NotifyWaitConditionLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaNotifyRole.Arn
      Code:
        ZipFile: !Sub |
          import json
          import boto3
          import urllib.request
          code_pipeline = boto3.client('codepipeline')
          def handler(event, context):
            job_id = event['CodePipeline.job']['id']
            url = '${DeployWaitHandle}'
            headers = { "Content-Type": "" }
            data = { "Status": "SUCCESS", "Reason": "Compilation Succeeded", "UniqueId": "Dask", "Data": "Compilation Succeeded" }
            try:
              req = urllib.request.Request(url, headers=headers, data=bytes(json.dumps(data), encoding="utf-8"), method='PUT')
              response = urllib.request.urlopen(req)
              code_pipeline.put_job_success_result(jobId=job_id)
            except Exception as e:
              print("failure: " + str(e))
              code_pipeline.put_job_failure_result(jobId=job_id, failureDetails={'message': str(e), 'type': 'JobFailed'})
      Runtime: python3.7
      Timeout: 10
  
  LambdaNotifyRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: root
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Resource: "*"
            Effect: Allow
            Action:
              - codepipeline:PutJobSuccessResult
              - codepipeline:PutJobFailureResult
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
            Resource: '*'

  EcrDockerRepository:
    Type: AWS::ECR::Repository

  CleanECROnDelete:
    DependsOn: CleanECROnDeleteLambda
    Type: Custom::CleanECR
    Properties:
      ServiceToken: 
        Fn::GetAtt: 
          - "CleanECROnDeleteLambda"
          - "Arn"
      RepositoryName: !Ref EcrDockerRepository

  CleanECROnDeleteLambda:
    DependsOn: EcrDockerRepository
    Type: "AWS::Lambda::Function"
    Properties:
      Code: 
        ZipFile: 
          !Sub |
            import json, boto3, logging
            import cfnresponse
            logger = logging.getLogger()
            logger.setLevel(logging.INFO)
    
            def lambda_handler(event, context):
                logger.info("event: {}".format(event))
                try:
                    repo = event['ResourceProperties']['RepositoryName']
                    logger.info("bucket: {}, event['RequestType']: {}".format(repo,event['RequestType']))
                    if event['RequestType'] == 'Delete':
                        client = boto3.client('ecr')
                        resp = client.list_images(repositoryName=repo)
                        if 'imageIds' in resp:
                          images = resp['imageIds']
                          image_ids = [imageDigest for imageDigest in images]
                          if len(image_ids) > 0:
                            client.batch_delete_image(repositoryName=repo, imageIds=image_ids)
    
                    sendResponseCfn(event, context, cfnresponse.SUCCESS)
                except Exception as e:
                    logger.info("Exception: {}".format(e))
                    sendResponseCfn(event, context, cfnresponse.FAILED)
    
            def sendResponseCfn(event, context, responseStatus):
                responseData = {}
                responseData['Data'] = {}
                cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")            
    
      Handler: "index.lambda_handler"
      Runtime: python3.7
      MemorySize: 128
      Timeout: 60
      Role : !GetAtt CleanECROnDeleteLambdaRole.Arn

  CleanECROnDeleteLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: !Join ['-', [!Ref 'AWS::StackName', 'CleanECROnDeleteLambdaPolicy'] ]
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
              - ecr:ListImages
              - ecr:batchDeleteImage
            Resource: !GetAtt EcrDockerRepository.Arn
          # - Effect: Allow
          #   Action:
          #   - logs:CreateLogGroup
          #   - logs:CreateLogStream
          #   - logs:PutLogEvents
          #   Resource: '*'

  DaskVpc:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      InstanceTenancy: default
      Tags:
        - Key: Name
          Value: dask-fargate/MyVpc
  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      CidrBlock: 10.0.0.0/18
      VpcId:
        Ref: DaskVpc
      AvailabilityZone:
        Fn::Select:
          - 0
          - Fn::GetAZs: ""
      MapPublicIpOnLaunch: true
  PublicSubnet1RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId:
        Ref: DaskVpc
      Tags:
        - Key: Name
          Value: dask-fargate/MyVpc/PublicSubnet1
  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId:
        Ref: PublicSubnet1RouteTable
      SubnetId:
        Ref: PublicSubnet1
  PublicSubnet1DefaultRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId:
        Ref: PublicSubnet1RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId:
        Ref: InternetGateway
    DependsOn:
      - VPCGatewayA
  PublicSubnet1EIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc
      Tags:
        - Key: Name
          Value: dask-fargate/MyVpc/PublicSubnet1
  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: dask-fargate/MyVpc
  VPCGatewayA:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId:
        Ref: DaskVpc
      InternetGatewayId:
        Ref: InternetGateway
  LogGroupScheduler:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/${AWS::StackName}/dask/scheduler
      RetentionInDays: 3
    UpdateReplacePolicy: Delete
    DeletionPolicy: Delete
  LogGroupWorker:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/${AWS::StackName}/dask/worker
      RetentionInDays: 3
    UpdateReplacePolicy: Delete
    DeletionPolicy: Delete
  ECSExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
        Version: "2012-10-17"
  ECSExecutionRolePolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyDocument:
        Statement:
          - Action:
              - ecr:BatchCheckLayerAvailability
              - ecr:GetDownloadUrlForLayer
              - ecr:BatchGetImage
              - ecr:GetAuthorizationToken
            Effect: Allow
            Resource: "*"
          - Action:
              - logs:CreateLogStream
              - logs:PutLogEvents
            Effect: Allow
            Resource:
              Fn::GetAtt:
                - LogGroupScheduler
                - Arn
          - Action:
              - logs:CreateLogStream
              - logs:PutLogEvents
            Effect: Allow
            Resource:
              Fn::GetAtt:
                - LogGroupWorker
                - Arn
        Version: "2012-10-17"
      PolicyName: ECSExecutionRolePolicy
      Roles:
        - Ref: ECSExecutionRole
  DaskCluster:
    Type: AWS::ECS::Cluster
    Properties:
      ClusterName: !Join ["-", [!Ref "AWS::StackName", "Fargate-Dask-Cluster"]]
  DaskClusterPrivateNS:
    Type: AWS::ServiceDiscovery::PrivateDnsNamespace
    Properties:
      Name: local-dask
      Vpc:
        Ref: DaskVpc
  SchedulerDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties:
      ContainerDefinitions:
        - Command:
            - dask-scheduler
          Cpu: 4096
          Essential: true
          Image: daskdev/dask:2.9.2
          LogConfiguration: 
            LogDriver: awslogs
            Options:
              awslogs-group:
                Ref: LogGroupScheduler
              awslogs-stream-prefix: ecs
              awslogs-region:
                Ref: AWS::Region
          Memory: 30720
          MemoryReservation: 30720
          Name: Worker
      Cpu: "4096"
      ExecutionRoleArn:
        Fn::GetAtt:
          - ECSExecutionRole
          - Arn
      Family: Dask-Scheduler
      Memory: "30720"
      NetworkMode: awsvpc
      RequiresCompatibilities:
        - FARGATE
      TaskRoleArn:
        Fn::GetAtt:
          - ECSExecutionRole
          - Arn
  WorkerDefinition:
    Type: AWS::ECS::TaskDefinition
    Properties:
      ContainerDefinitions:
        - Command:
            - dask-worker
            - dask-scheduler.local-dask:8786
            - --memory-limit
            - '7000MB'
            - --worker-port
            - '9000'
            - --no-nanny
            - --no-dashboard
            - --death-timeout
            - '600'
            - --nthreads
            - '1'
            - --nprocs
            - '1'
            - --reconnect
          Cpu: 1024
          Essential: true
          Image: daskdev/dask:2.19.0
          LogConfiguration:
            LogDriver: awslogs
            Options:
              awslogs-group:
                Ref: LogGroupWorker
              awslogs-stream-prefix: ecs
              awslogs-region:
                Ref: AWS::Region
          Memory: 8192
          MemoryReservation: 8192
          Name: Worker
      Cpu: "1024"
      ExecutionRoleArn:
        Fn::GetAtt:
          - ECSExecutionRole
          - Arn
      Family: Dask-Worker
      Memory: "8192"
      NetworkMode: awsvpc
      RequiresCompatibilities:
        - FARGATE
      TaskRoleArn:
        Fn::GetAtt:
          - ECSExecutionRole
          - Arn
  DaskSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable Scheduler ports access
      GroupName: DaskSecurityGroup
      SecurityGroupEgress:
        - CidrIp: 0.0.0.0/0
          Description: Allow all outbound traffic by default
          IpProtocol: "-1"
      SecurityGroupIngress:
        - CidrIp: 0.0.0.0/0
          Description: from 0.0.0.0/0:p1
          FromPort: 8786
          IpProtocol: tcp
          ToPort: 8789
        - CidrIp: 0.0.0.0/0
          Description: from 0.0.0.0/0:p2
          FromPort: 9000
          IpProtocol: tcp
          ToPort: 9002
      VpcId:
        Ref: DaskVpc
  DaskSchedulerService:
    Type: AWS::ECS::Service
    Properties:
      Cluster:
        Ref: DaskCluster
      DeploymentConfiguration:
        MaximumPercent: 200
        MinimumHealthyPercent: 100
      DesiredCount: 1
      EnableECSManagedTags: false
      LaunchType: FARGATE
      NetworkConfiguration:
        AwsvpcConfiguration:
          AssignPublicIp: ENABLED
          SecurityGroups:
            - Fn::GetAtt:
                - DaskSecurityGroup
                - GroupId
          Subnets:
            - Ref: PublicSubnet1
      ServiceName: Dask-Scheduler
      ServiceRegistries:
        - RegistryArn:
            Fn::GetAtt:
              - DaskSchedulerServiceDiscovery
              - Arn
      TaskDefinition:
        Ref: SchedulerDefinition
  DaskSchedulerServiceDiscovery:
    Type: AWS::ServiceDiscovery::Service
    Properties:
      DnsConfig:
        DnsRecords:
          - TTL: 5
            Type: A
        NamespaceId:
          Fn::GetAtt:
            - DaskClusterPrivateNS
            - Id
        RoutingPolicy: MULTIVALUE
      HealthCheckCustomConfig:
        FailureThreshold: 1
      Name: Dask-Scheduler
      NamespaceId:
        Fn::GetAtt:
          - DaskClusterPrivateNS
          - Id
  DaskWorkerService:
    Type: AWS::ECS::Service
    Properties:
      Cluster:
        Ref: DaskCluster
      DeploymentConfiguration:
        MaximumPercent: 200
        MinimumHealthyPercent: 100
      DesiredCount: 0
      EnableECSManagedTags: false
      LaunchType: FARGATE
      NetworkConfiguration:
        AwsvpcConfiguration:
          AssignPublicIp: ENABLED
          SecurityGroups:
            - Fn::GetAtt:
                - DaskSecurityGroup
                - GroupId
          Subnets:
            - Ref: PublicSubnet1
      ServiceName: Dask-Worker
      TaskDefinition:
        Ref: WorkerDefinition

  SagemakerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: sagemaker.amazonaws.com
        Version: "2012-10-17"
  SagemakerPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyDocument:
        Statement:
          - Action:
              - s3:*
              - ecs:*
              - logs:*
            Effect: Allow
            Resource: "*"
        Version: "2012-10-17"
      PolicyName: notebookAccessPolicy
      Roles:
        - Ref: SagemakerRole

  NotebookLifecycleConfig:
    Type: AWS::SageMaker::NotebookInstanceLifecycleConfig
    Properties:
        OnCreate:
        - Content:
            Fn::Base64: |
                #!/bin/sh

                set -e
                cat > /home/ec2-user/setup.sh << EOF
                #!/bin/bash
                sleep 10
                echo "Creating daskpy3 conda environment"
                conda create --name daskpy3 python="3.7.9" -y
                source activate daskpy3
                conda install -c conda basemap proj4 -y 
                pip install zarr==2.3.2 rechunker==0.3.2 ipykernel boto3 dask==2.19.0 distributed==2.19.0 tornado==6.0.4 cloudpickle==1.4.1 msgpack==0.6.2 blosc==1.8.1 numpy==1.17.3 lz4==3.0.2 netcdf4==1.5.3 xarray==0.16.2 bokeh==2.1.1 s3fs==0.5.2 fsspec==0.8.5 h5netcdf==0.8.0 h5py==2.10.0 intake-stac==0.3.0 sat-search==0.3.0 rioxarray==0.1.1 geopandas==0.8.1
                python -m ipykernel install --user --name daskpy3 --display-name "conda_daskpy3"
                echo "Updating Jupyter config options"
                sed -i.bak 's/^#c.NotebookApp.iopub_data_rate_limit.*$/c.NotebookApp.iopub_data_rate_limit = 1.0e10/' ~/.jupyter/jupyter_notebook_config.py
                echo "Finished!"
                EOF
            
                chown ec2-user:ec2-user /home/ec2-user/setup.sh
                chmod +x /home/ec2-user/setup.sh

                sudo -u ec2-user -i nohup /home/ec2-user/setup.sh >/home/ec2-user/output.log 2>&1 &

  DaskNotebook:
    Type: AWS::SageMaker::NotebookInstance
    Properties:
      InstanceType: ml.t3.2xlarge
      RoleArn:
        Fn::GetAtt:
          - SagemakerRole
          - Arn
      DefaultCodeRepository: !Ref SagemakerCodeRepo
      DirectInternetAccess: Enabled
      NotebookInstanceName: !Join ['-', [!Ref 'AWS::StackName', 'DaskNotebook'] ]
      LifecycleConfigName:  !GetAtt NotebookLifecycleConfig.NotebookInstanceLifecycleConfigName
      RootAccess: Enabled
      SecurityGroupIds:
        - Fn::GetAtt:
            - DaskSecurityGroup
            - GroupId
      SubnetId:
        Ref: PublicSubnet1
      VolumeSizeInGB: 50

Outputs:
  JupyterNotebook:
      Value: !Join
      - ''
      - - https://
        - !Ref 'AWS::Region'
        - .console.aws.amazon.com/sagemaker/home?region=
        - !Ref 'AWS::Region'
        - '#/notebook-instances/openNotebook/'
        - !GetAtt 'DaskNotebook.NotebookInstanceName'
        - '?view=lab'
